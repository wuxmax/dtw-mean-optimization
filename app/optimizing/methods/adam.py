import numpy as np
from optimizing.interface import get_subgradient
from optimizing.dtw_mean import frechet

"""
Based on:

Adam: A Method for Stochastic Optimization
https://arxiv.org/abs/1412.6980

Require: Î±: Stepsize
Require: Î²1, Î²2 âˆˆ [0, 1): Exponential decay rates for the moment estimates
Require: f(Î¸): Stochastic objective function with parameters Î¸
Require: Î¸0: Initial parameter vector
m0 â† 0 (Initialize 1st moment vector)
v0 â† 0 (Initialize 2nd moment vector)
t â† 0 (Initialize timestep)
while Î¸t not converged do
    t â† t + 1
    gt â† âˆ‡Î¸ft(Î¸tâˆ’1) (Get gradients w.r.t. stochastic objective at timestep t)
    mt â† Î²1 Â· mtâˆ’1 + (1 âˆ’ Î²1) Â· gt (Update biased first moment estimate)
    vt â† Î²2 Â· vtâˆ’1 + (1 âˆ’ Î²2) Â· g2t (Update biased second raw moment estimate)
    mbt â† mt/(1 âˆ’ Î²t1) (Compute bias-corrected first moment estimate)
    vbt â† vt/(1 âˆ’ Î²t2) (Compute bias-corrected second raw moment estimate)
    Î¸t â† Î¸tâˆ’1 âˆ’ Î± Â· mb t/(âˆšvbt + eps) (Update parameters)
end while
return Î¸t (Resulting parameters)
"""

def run(X, z, f, batch_size, n_coverage, n_epochs, d_converged, rng):
    N = X.shape[0]  # number of data point
    d = z.shape     # dimensions of data

    # number of update steps
    n_steps = int(np.ceil(n_coverage / batch_size))

    # initialize adam parameters 
    # (as it was done in the paper)
    alpha = 0.001
    beta1 = 0.9
    beta2 = 0.999
    eps_stable = 1e-8

    # initialize momentum estimates
    m = np.zeros((n_steps + 1,) + d)
    v = np.zeros((n_steps + 1,) + d)

    # counter for visited samples and update steps
    n_visited_samples = 0
    t = 0
    
    for k in range(n_epochs):
        # shuffle data indices for new epoch
        perm = rng.permutation(N)

        for i in range(0, N, batch_size):
                        
            # break if number of samples to visit is reached or exceeded
            if not n_visited_samples < n_coverage:
                break
            
            # break if there is not an entire batch left 
            # (relevant for batch_size > 1)
            if N - i < batch_size:
                break

            # update step index (+1 indexed, because of m, v initialization)
            t += 1

            # get subgradients for current steps
            g = get_subgradient(X, z, i, batch_size, perm)

            # compute biased momentum estimates
            m[t] = beta1 * m[t - 1] + (1 - beta1) * g
            v[t] = beta2 * v[t - 1] + (1 - beta2) * np.square(g)

            # compute bias-corrected momentum estimates
            m_ = m[t] / (1 - np.power(beta1, t))
            v_ = v[t] / (1 - np.power(beta2, t))
            
            # actual update step
            z = z - alpha * m_ / (np.sqrt(v_) + eps_stable)

            # update number of visited samples
            n_visited_samples += batch_size
        
        # f[0] is initial value, therefore +1 indexed
        f[k + 1] = frechet(z, X)

        # check if current z is best
        if np.amin(f) == f[k + 1]:
            z_ = z

        # stop if converged
        f_diff = abs((f[k + 1] -  f[k]) / f[k])
        if f_diff < d_converged:
            break

    return z_, f

        